{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metric_Function.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPRo5-2Ji7H0",
        "colab_type": "text"
      },
      "source": [
        "# Identificação de Alzaimer em imagens RX\n",
        "\n",
        "* Mohammad Reza Tabrizi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dpyI0HcMpeN",
        "colab_type": "text"
      },
      "source": [
        "### definição de Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I7m-eCPMpeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callbacks definition\n",
        "early = EarlyStopping(monitor='val_accuracy', min_delta=0,patience= 10,verbose= True, mode='auto')\n",
        "#Checkpoint=ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "LROnPlateau=ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001,\n",
        "                              cooldown=0, min_lr=0)\n",
        "callbacks = [early,LROnPlateau]\n",
        "history_pickle=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HRlHMrdMpeT",
        "colab_type": "text"
      },
      "source": [
        "### Definição de funções auxiliares\n",
        "#### Começamos por apresentar métricas que enriquecerão o report dos modelos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPpy7BPMpeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(label, confusion_matrix):\n",
        "    col = confusion_matrix[:, label]\n",
        "    return confusion_matrix[label, label] / col.sum()\n",
        "    \n",
        "def recall(label, confusion_matrix):\n",
        "    row = confusion_matrix[label, :]\n",
        "    return confusion_matrix[label, label] / row.sum()\n",
        "\n",
        "def precision_macro_average(confusion_matrix):\n",
        "    rows, columns = confusion_matrix.shape\n",
        "    sum_of_precisions = 0\n",
        "    for label in range(rows):\n",
        "        sum_of_precisions += precision(label, confusion_matrix)\n",
        "    return sum_of_precisions / rows\n",
        "\n",
        "def recall_macro_average(confusion_matrix):\n",
        "    rows, columns = confusion_matrix.shape\n",
        "    sum_of_recalls = 0\n",
        "    for label in range(columns):\n",
        "        sum_of_recalls += recall(label, confusion_matrix)\n",
        "    return sum_of_recalls / columns\n",
        "\n",
        "def accuracy(confusion_matrix):\n",
        "    diagonal_sum = confusion_matrix.trace()\n",
        "    sum_of_all_elements = confusion_matrix.sum()\n",
        "    return diagonal_sum / sum_of_all_elements \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbWr8i1_MpeY",
        "colab_type": "text"
      },
      "source": [
        "#### As seguintes funções avaliam, para cada modelo, a matriz de confusão, métricas de performance e plots da loss e accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsxRy7IkMpeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_metrics(model,history,X_test_All, y_test_All):\n",
        "   model.save('model.--{epoch:02d}-{val_accuracy:.2f}.h5')\n",
        "   filename = 'model.--{epoch:02d}-{val_accuracy:.2f}.sav'\n",
        "   joblib.dump(model, filename)\n",
        "\n",
        "   cm = confusion_matrix(y_test, np.argmax(model.predict(X_test_All),axis=1))\n",
        "   conf_mat=pd.DataFrame(cm)\n",
        "   conf_mat.index.name='Actual'\n",
        "   conf_mat.columns.name='Predicted'\n",
        "\n",
        "   print(conf_mat)\n",
        "   print('accuracy total:', accuracy(cm))\n",
        "   print('precision NonDemente:', precision(0,cm))\n",
        "   print('recall NonDemente:', recall(0,cm))\n",
        "   print('precision ModerateDemented:', precision(1,cm))\n",
        "   print('recall ModerateDemented:', recall(1,cm))\n",
        "   print('precision MildDemented:', precision(2,cm))\n",
        "   print('recall MildDemented:', recall(2,cm))\n",
        "   print('precision VeryMildDemented:', precision(3,cm))\n",
        "   print('recall VeryMildDemented:', recall(3,cm))\n",
        "   print('precision total:', precision_macro_average(cm))\n",
        "   print('recall total:', recall_macro_average(cm))\n",
        "\n",
        "   print(\"label precision recall\")\n",
        "   for label in range(len(unique_labels(y_test_All))):\n",
        "    print(f\"{label:5d} {precision(label, cm):9.3f} {recall(label, cm):6.3f}\")\n",
        "\n",
        "   results=model.evaluate(X_test_All, y_test_All)\n",
        "   print(results)\n",
        "\n",
        "   plt.figure(figsize=(12,4))\n",
        "   plt.subplot(1, 2, 1)\n",
        "   plt.plot(history.history['accuracy'])\n",
        "   plt.plot(history.history['val_accuracy'])\n",
        "   plt.title('Model accuracy')\n",
        "   plt.ylabel('Accuracy')\n",
        "   plt.xlabel('Epoch')\n",
        "   plt.legend(['Train', 'Test'], loc='upper left')\n",
        "   plt.subplot(1, 2, 2)\n",
        "   plt.plot(history.history['loss'])\n",
        "   plt.plot(history.history['val_loss'])\n",
        "   plt.title('Model loss')\n",
        "   plt.ylabel('Loss')\n",
        "   plt.xlabel('Epoch')\n",
        "   plt.legend(['Train', 'Test'], loc='upper left')\n",
        "   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6tgCxlWUg2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_metrics2(model,history,X_test_All, y_test_All):\n",
        "   model.save('model.--{epoch:02d}-{val_accuracy:.2f}.h5')\n",
        "   filename = 'model.--{epoch:02d}-{val_accuracy:.2f}.sav'\n",
        "   joblib.dump(model, filename)\n",
        "\n",
        "   cm = confusion_matrix(y_test, np.argmax(model.predict(X_test_All),axis=1))\n",
        "   conf_mat=pd.DataFrame(cm)\n",
        "   conf_mat.index.name='Actual'\n",
        "   conf_mat.columns.name='Predicted'\n",
        "\n",
        "   print(conf_mat)\n",
        "   print('accuracy total:', accuracy(cm))\n",
        "   print('precision NonDemente:', precision(0,cm))\n",
        "   print('recall NonDemente:', recall(0,cm))\n",
        "   print('precision ModerateDemented:', precision(1,cm))\n",
        "   print('recall ModerateDemented:', recall(1,cm))\n",
        "   print('precision MildDemented:', precision(2,cm))\n",
        "   print('recall MildDemented:', recall(2,cm))\n",
        "   print('precision VeryMildDemented:', precision(3,cm))\n",
        "   print('recall VeryMildDemented:', recall(3,cm))\n",
        "   print('precision total:', precision_macro_average(cm))\n",
        "   print('recall total:', recall_macro_average(cm))\n",
        "\n",
        "   print(\"label precision recall\")\n",
        "   for label in range(len(unique_labels(y_test_All))):\n",
        "    print(f\"{label:5d} {precision(label, cm):9.3f} {recall(label, cm):6.3f}\")\n",
        "\n",
        "   results=model.evaluate(X_test_All, y_test_All)\n",
        "   print(results)\n",
        "\n",
        "   plt.figure(figsize=(12,4))\n",
        "   plt.subplot(1, 2, 1)\n",
        "   plt.plot(history.history['accuracy'])\n",
        "   #plt.plot(history.history['val_accuracy'])\n",
        "   plt.title('Model accuracy')\n",
        "   plt.ylabel('Accuracy')\n",
        "   plt.xlabel('Epoch')\n",
        "   plt.legend(['Train', 'Test'], loc='upper left')\n",
        "   plt.subplot(1, 2, 2)\n",
        "   plt.plot(history.history['loss'])\n",
        "   #plt.plot(history.history['val_loss'])\n",
        "   plt.title('Model loss')\n",
        "   plt.ylabel('Loss')\n",
        "   plt.xlabel('Epoch')\n",
        "   plt.legend(['Train', 'Test'], loc='upper left')\n",
        "   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}